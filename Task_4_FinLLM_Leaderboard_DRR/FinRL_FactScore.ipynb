{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/shmsw25/FActScore.git"
      ],
      "metadata": {
        "id": "rTJdD7Z6dw8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.27.0"
      ],
      "metadata": {
        "id": "z3geH7ppKEH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r FActScore/requirements.txt"
      ],
      "metadata": {
        "id": "R_RsJXDseDMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "eLxESzoXe0Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip demos.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/demos.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/FActScore/factscore')"
      ],
      "metadata": {
        "id": "kfKO6l1Z6Uro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "sys.path.append('/content/FActScore')\n",
        "from factscore.factscorer import FactScorer"
      ],
      "metadata": {
        "id": "v9Q6NPGfTO00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "# Fill your openai key here\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "ko6rHAX1V9jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "CYKeXDWogBd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "os.makedirs('.cache/factscore/demos/', exist_ok=True)\n",
        "shutil.copy('./FActScore/factscore/demos/demons.json',\n",
        "            '.cache/factscore/demos/demons.json')"
      ],
      "metadata": {
        "id": "Vjx_zaeG8KKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input data for FACTSCORE\n",
        "input_csv = '/content/.csv'  # Replace with the file name you want to test\n",
        "df = pd.read_csv(input_csv)\n",
        "input_data = []\n",
        "for i, row in df.iterrows():\n",
        "    # the title of question column should be changed to \"Question\", and the answer column should be changed to \"Generated Answer\"\n",
        "    question = row[\"Question\"]\n",
        "    generated_answer = row[\"Generated Answer\"]\n",
        "\n",
        "    def convert_datetime(value):\n",
        "        if isinstance(value, (pd.Timestamp, datetime)):\n",
        "            return value.isoformat()\n",
        "        return value\n",
        "\n",
        "    question = convert_datetime(question)\n",
        "    generated_answer = convert_datetime(generated_answer)\n",
        "\n",
        "    input_data.append({\n",
        "        \"question\": question,\n",
        "        \"generated_answer\": generated_answer\n",
        "    })"
      ],
      "metadata": {
        "id": "CT_S7Rf2isHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write input to a JSONL file, which FACTSCORE expects\n",
        "with open(\"input.jsonl\", \"w\") as f:\n",
        "    for item in input_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "bRflXNNUis3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fs = FactScorer()\n",
        "\n",
        "# upload konwledge_base.jsonl first\n",
        "name_of_your_knowledge_source = 'Q&A'\n",
        "fs.register_knowledge_source(name_of_your_knowledge_source, data_path='/content/knowledge_base.jsonl', db_path='input.db')"
      ],
      "metadata": {
        "id": "7wC7zDdEePNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [item[\"question\"] for item in input_data]\n",
        "generations = [item[\"generated_answer\"] for item in input_data]"
      ],
      "metadata": {
        "id": "mEngmh2jfZzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "-tuGmxCjDOB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# testing\n",
        "test_text = \"This is a test sentence. Let's see if the tokenizer works correctly.\"\n",
        "print(sent_tokenize(test_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddLoPH0B5Cc",
        "outputId": "14235f8e-3051-4d52-d2dd-6b0121f8602b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a test sentence.', \"Let's see if the tokenizer works correctly.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your openai api key first\n",
        "with open(\"/content/api.key\", \"w\") as f:\n",
        "    f.write(\"\")\n"
      ],
      "metadata": {
        "id": "O1EdnNewIJhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "# set up path to store the factscore result first\n",
        "excel_path = \".xlsx\"\n",
        "pd.DataFrame(columns=['Question', 'Score', 'Init Score', 'Respond Ratio', 'Num Facts']).to_excel(excel_path, index=False)\n",
        "\n",
        "score = 0\n",
        "init_score = 0\n",
        "num_facts = 0\n",
        "respond_ratio = 0\n",
        "\n",
        "for i, item in enumerate(input_data):\n",
        "    conn = sqlite3.connect(\"/content/input.db\")\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    fs.db[name_of_your_knowledge_source].connection = conn\n",
        "\n",
        "    out = fs.get_score([item[\"question\"]], [item[\"generated_answer\"]], knowledge_source=name_of_your_knowledge_source)\n",
        "\n",
        "    new_row = pd.DataFrame({\n",
        "        'Question': [item[\"question\"]],\n",
        "        'Score': [out[\"score\"]],\n",
        "        'Init Score': [out[\"init_score\"]],\n",
        "        'Respond Ratio': [out[\"respond_ratio\"]],\n",
        "        'Num Facts': [out[\"num_facts_per_response\"]]\n",
        "    })\n",
        "\n",
        "    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
        "        startrow = writer.sheets['Sheet1'].max_row\n",
        "        new_row.to_excel(writer, index=False, header=False, startrow=startrow)\n",
        "\n",
        "    print(f\"Question {i+1} saved\")\n",
        "\n",
        "    score += out[\"score\"] * out[\"num_facts_per_response\"]\n",
        "    init_score += out[\"init_score\"] * out[\"num_facts_per_response\"]\n",
        "    respond_ratio += out[\"respond_ratio\"]\n",
        "    num_facts += out[\"num_facts_per_response\"]\n",
        "\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'Question': ['Summary'],\n",
        "    'Score': [score / num_facts],\n",
        "    'Init Score': [init_score / num_facts],\n",
        "    'Respond Ratio': [respond_ratio / len(input_data)],\n",
        "    'Num Facts': [num_facts / len(input_data)]\n",
        "})\n",
        "\n",
        "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
        "    startrow = writer.sheets['Sheet1'].max_row\n",
        "    summary.to_excel(writer, index=False, header=False, startrow=startrow)\n",
        "\n",
        "print(\"Summary saved\")"
      ],
      "metadata": {
        "id": "GMxOL7SzgSwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}