{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbdEElE7kw1y",
        "outputId": "13eb121e-e48b-4907-b6b8-7f015d4ea09d"
      },
      "outputs": [],
      "source": [
        "!pip install peft torch\n",
        "!pip install mistralai together\n",
        "!pip install openai==0.28\n",
        "!pip install friendli-client\n",
        "!pip install transformers==4.45.1\n",
        "!pip install tiktoken\n",
        "!pip install transformers_stream_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDdwM3WPWV4p"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import logging\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from mistralai import Mistral\n",
        "from together import Together\n",
        "from huggingface_hub import login\n",
        "from friendli import Friendli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mbabF0-WZuO"
      },
      "outputs": [],
      "source": [
        "# Suppress all warnings from transformers\n",
        "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhBkGwH1Wce2",
        "outputId": "b0510a7a-9ed9-4b8a-e69b-bfd2fc0f913c"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available and set device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Using GPU device {device}.\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy-Jad-cWf7o"
      },
      "outputs": [],
      "source": [
        "# Load API keys from config\n",
        "with open('config.json', 'r') as config_file:\n",
        "    config = json.load(config_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = config.get('openai_api_key', None)\n",
        "MISTRAL_API_KEY = config.get('mistral_api_key', None)\n",
        "TOGETHER_API_KEY = config.get('together_ai_api_key', None)\n",
        "HF_API_KEY = config.get('huggingface_api_key', None)\n",
        "FRIENDLI_API_TOKEN = config.get('friendli_api_token', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "login(token=HF_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5lUxiV9Wv3b"
      },
      "outputs": [],
      "source": [
        "# Ollama REST API endpoint (if using Ollama)\n",
        "OLLAMA_API_URL = 'http://localhost:11434/api/generate'\n",
        "\n",
        "# Initialize the Mistral client\n",
        "mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n",
        "\n",
        "# Initialize the Together client\n",
        "together_client = Together(api_key=TOGETHER_API_KEY)\n",
        "\n",
        "# Initialize the OpenAI GPT-4o client is handled via the transformers library\n",
        "\n",
        "# Define the model configurations\n",
        "MODEL_CONFIGS = [\n",
        "    # {\n",
        "    #     \"name\": \"GPT-4o\",\n",
        "    #     \"type\": \"openai\",\n",
        "    #     \"model_name\": \"gpt-4o\",  # OpenAI's GPT-4o identifier\n",
        "    #     \"api\": \"openai\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"Mistral_Large_2\",\n",
        "    #     \"type\": \"mistral\",\n",
        "    #     \"model_name\": \"mistral-large-latest\",\n",
        "    #     \"api\": \"mistral\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"name\": \"LLaMA_3.1-8B\",\n",
        "    #     \"type\": \"llama\",\n",
        "    #     \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    #     \"api\": \"together\"\n",
        "    # },\n",
        "]\n",
        "\n",
        "# Function to initialize models\n",
        "def initialize_models(model_configs, device):\n",
        "    \"\"\"Initialize models based on the provided configurations.\"\"\"\n",
        "    initialized_models = []\n",
        "    for config in model_configs:\n",
        "        try:\n",
        "            api_type = config.get(\"api\", \"unknown\")\n",
        "            \n",
        "            if api_type == \"openai\":\n",
        "                print(f\"Setting up OpenAI for {config['name']}...\")\n",
        "                initialized_models.append(config)\n",
        "            elif api_type == \"mistral\":\n",
        "                print(f\"Setting up Mistral for {config['name']}...\")\n",
        "                initialized_models.append(config)\n",
        "            elif api_type == \"together\":\n",
        "                print(f\"Setting up Together AI for {config['name']}...\")\n",
        "                initialized_models.append(config)\n",
        "            elif api_type == \"huggingface\":\n",
        "                print(f\"Setting up HuggingFace model for {config['name']}...\")\n",
        "                initialized_models.append(config)\n",
        "            elif api_type == \"friendli\":\n",
        "                endpoint_type = config.get(\"endpoint_type\", \"serverless\")\n",
        "                if endpoint_type == \"serverless\":\n",
        "                    print(f\"Setting up Friendli serverless model for {config['name']}...\")\n",
        "                elif endpoint_type == \"dedicated\":\n",
        "                    print(f\"Setting up Friendli dedicated endpoint for {config['name']}...\")\n",
        "                elif endpoint_type == \"container\":\n",
        "                    print(f\"Setting up Friendli container for {config['name']}...\")\n",
        "                initialized_models.append(config)\n",
        "            else:\n",
        "                print(f\"Unknown API type for model {config['name']}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up model {config['name']}: {e}\")\n",
        "    return initialized_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba_mXQanWxxl"
      },
      "outputs": [],
      "source": [
        "# Define the inference engine for OpenAI GPT-4o\n",
        "class OpenAIInferenceEngine:\n",
        "    def __init__(self, model_name=\"gpt-4o\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate_response(self, system_prompt, user_prompt):\n",
        "        \"\"\"\n",
        "        Generate a response using OpenAI's GPT-4o.\n",
        "\n",
        "        Args:\n",
        "            system_prompt (str): System-level instructions.\n",
        "            user_prompt (str): User's input or question.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response from GPT-4o.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=1024,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0\n",
        "            )\n",
        "            answer = response['choices'][0]['message']['content'].strip()\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response with {self.model_name}: {e}\")\n",
        "            return \"Error generating response.\"\n",
        "\n",
        "# Define the inference engine for Mistral Large 2\n",
        "class MistralInferenceEngine:\n",
        "    def __init__(self, client, model_name=\"mistral-large-latest\"):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate_response(self, system_prompt, user_prompt):\n",
        "        \"\"\"\n",
        "        Generate a response using Mistral Large 2.\n",
        "\n",
        "        Args:\n",
        "            system_prompt (str): System-level instructions.\n",
        "            user_prompt (str): User's input or question.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response from Mistral Large 2.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
        "            response = self.client.chat.complete(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=1024,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0\n",
        "            )\n",
        "            answer = response.choices[0].message.content.strip()\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response with {self.model_name}: {e}\")\n",
        "            return \"Error generating response.\"\n",
        "\n",
        "# Define the inference engine for LLaMA 3.1-8B via Together AI\n",
        "class LLaMAInferenceEngine:\n",
        "    def __init__(self, client, model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def generate_response(self, system_prompt, user_prompt):\n",
        "        \"\"\"\n",
        "        Generate a response using LLaMA 3.1-8B via Together AI.\n",
        "\n",
        "        Args:\n",
        "            system_prompt (str): System-level instructions.\n",
        "            user_prompt (str): User's input or question.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response from LLaMA 3.1-8B.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0,\n",
        "                max_tokens=1024,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0\n",
        "            )\n",
        "            answer = response.choices[0].message.content.strip()\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response with {self.model_name}: {e}\")\n",
        "            return \"Error generating response.\"\n",
        "        \n",
        "# HuggingFace direct model inference engine\n",
        "class HuggingFaceInferenceEngine:\n",
        "   def __init__(self, model_path, device=\"cuda\"):\n",
        "       \"\"\"\n",
        "       Initialize a Hugging Face model directly from the provided path.\n",
        "      \n",
        "       Args:\n",
        "           model_path (str): The path or identifier of the model on HuggingFace Hub.\n",
        "           device (str): The device to load the model on (\"cuda\" or \"cpu\").\n",
        "       \"\"\"\n",
        "       self.model_path = model_path\n",
        "       self.device = device\n",
        "       print(f\"Loading HuggingFace model from {model_path}...\")\n",
        "      \n",
        "       # Two options for loading - pipeline (easier) or direct model loading (more customizable)\n",
        "       # Option 1: Using pipeline (simpler)\n",
        "       self.pipe = pipeline(\n",
        "           \"text-generation\",\n",
        "           model=model_path,\n",
        "           device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
        "           max_new_tokens=1024\n",
        "       )\n",
        "      \n",
        "       # Option 2: Direct model and tokenizer loading (more control)\n",
        "       self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "       self.model = AutoModelForCausalLM.from_pretrained(\n",
        "           model_path,\n",
        "           device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
        "           torch_dtype=\"auto\"\n",
        "       )\n",
        "       print(f\"Successfully loaded model from {model_path}\")\n",
        "\n",
        "   def generate_response(self, system_prompt, user_prompt):\n",
        "       \"\"\"\n",
        "       Generate a response using the HuggingFace model.\n",
        "      \n",
        "       Args:\n",
        "           system_prompt (str): System-level instructions.\n",
        "           user_prompt (str): User's input or question.\n",
        "          \n",
        "       Returns:\n",
        "           str: Generated response from the model.\n",
        "       \"\"\"\n",
        "       try:\n",
        "           # Prepare messages in the format expected by the model\n",
        "           messages = [\n",
        "               {\"role\": \"system\", \"content\": system_prompt},\n",
        "               {\"role\": \"user\", \"content\": user_prompt}\n",
        "           ]\n",
        "          \n",
        "           # Generate using the pipeline\n",
        "           result = self.pipe(messages, do_sample=True, temperature=0, top_p=1.0)\n",
        "          \n",
        "           # Extract the generated text from the result\n",
        "           if isinstance(result, list) and len(result) > 0:\n",
        "               if \"generated_text\" in result[0]:\n",
        "                   # Standard output format\n",
        "                   response = result[0][\"generated_text\"]\n",
        "               else:\n",
        "                   # Extract the last message content if it's a conversation\n",
        "                   response = result[0]\n",
        "           else:\n",
        "               response = str(result)\n",
        "              \n",
        "           # Clean up the response to get just the assistant's reply\n",
        "           # This may need adjustment based on the exact model's output format\n",
        "           if isinstance(response, str) and \"assistant\" in response.lower():\n",
        "               # Try to extract just the assistant's part\n",
        "               parts = response.split(\"assistant\")\n",
        "               if len(parts) > 1:\n",
        "                   response = parts[1].strip()\n",
        "                   if response.startswith(\":\"):\n",
        "                       response = response[1:].strip()\n",
        "          \n",
        "           return response.strip()\n",
        "      \n",
        "       except Exception as e:\n",
        "           print(f\"Error generating response with HuggingFace model: {e}\")\n",
        "           return f\"Error generating response: {str(e)}\"\n",
        "       \n",
        "# FriendliInferenceEngine class for handling all Friendli deployment types\n",
        "class FriendliInferenceEngine:\n",
        "    def __init__(self, token, model_name=None, endpoint_id=None, team_id=None, \n",
        "                 adapter_route=None, base_url=None, use_container=False):\n",
        "        \"\"\"\n",
        "        Initialize the appropriate Friendli client based on deployment type.\n",
        "        \n",
        "        Args:\n",
        "            token: Friendli API token (not needed for container mode)\n",
        "            model_name: Name for serverless models (e.g., meta-llama-3.1-8b-instruct)\n",
        "            endpoint_id: ID for dedicated endpoint custom models\n",
        "            team_id: Team ID for dedicated endpoints\n",
        "            adapter_route: Adapter route for Multi-LoRA models\n",
        "            base_url: URL for container deployment\n",
        "            use_container: Flag for container mode\n",
        "        \"\"\"\n",
        "        \n",
        "        # Store parameters\n",
        "        self.token = token\n",
        "        self.model_name = model_name\n",
        "        self.endpoint_id = endpoint_id\n",
        "        self.adapter_route = adapter_route\n",
        "        self.use_container = use_container\n",
        "        \n",
        "        # Container mode (self-hosted)\n",
        "        if use_container:\n",
        "            self.client = Friendli(base_url=base_url or \"http://0.0.0.0:8000\")\n",
        "            self.model_identifier = adapter_route if adapter_route else None\n",
        "            print(f\"Using Friendli Container at {base_url or 'http://0.0.0.0:8000'}\")\n",
        "            \n",
        "        # Dedicated endpoint mode (custom models)\n",
        "        elif endpoint_id:\n",
        "            self.client = Friendli(token=token, team_id=team_id, use_dedicated_endpoint=True)\n",
        "            if adapter_route:\n",
        "                # Multi-LoRA model\n",
        "                self.model_identifier = f\"{endpoint_id}:{adapter_route}\"\n",
        "            else:\n",
        "                # Regular dedicated endpoint\n",
        "                self.model_identifier = endpoint_id\n",
        "            print(f\"Using Friendli Dedicated Endpoint: {self.model_identifier}\")\n",
        "            \n",
        "        # Serverless mode (public models like Llama)\n",
        "        else:\n",
        "            self.client = Friendli(token=token)\n",
        "            self.model_identifier = model_name\n",
        "            print(f\"Using Friendli Serverless model: {model_name}\")\n",
        "    \n",
        "    def generate_response(self, system_prompt, user_prompt):\n",
        "        \"\"\"Generate a response from the Friendli model.\"\"\"\n",
        "        try:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "            \n",
        "            # Handle different API endpoints based on deployment type\n",
        "            if self.adapter_route and not self.use_container:\n",
        "                # Multi-LoRA models on dedicated endpoints use the lora API\n",
        "                response = self.client.lora.completions.create(\n",
        "                    model=self.model_identifier,\n",
        "                    messages=messages,\n",
        "                    temperature=0,\n",
        "                    max_tokens=1024\n",
        "                )\n",
        "            else:\n",
        "                # Standard chat completions API for all other cases\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model_identifier if self.model_identifier else None,\n",
        "                    messages=messages,\n",
        "                    temperature=0,\n",
        "                    max_tokens=1024\n",
        "                )\n",
        "            \n",
        "            answer = response.choices[0].message.content.strip()\n",
        "            return answer\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error with Friendli model: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close the client to release resources.\"\"\"\n",
        "        if hasattr(self, 'client'):\n",
        "            self.client.close()\n",
        "# Define the InferenceEngine wrapper\n",
        "class InferenceEngine:\n",
        "    def __init__(self, model_info):\n",
        "        \"\"\"Initialize the appropriate inference engine based on model configuration.\"\"\"\n",
        "        self.name = model_info[\"name\"]\n",
        "        self.type = model_info[\"type\"]\n",
        "        self.api = model_info.get(\"api\", None)\n",
        "        \n",
        "        if self.api == \"openai\":\n",
        "            self.model_name = model_info[\"model_name\"]\n",
        "            self.engine = OpenAIInferenceEngine(model_name=self.model_name)\n",
        "        elif self.api == \"mistral\":\n",
        "            self.model_name = model_info[\"model_name\"]\n",
        "            self.engine = MistralInferenceEngine(client=mistral_client, model_name=self.model_name)\n",
        "        elif self.api == \"together\":\n",
        "            self.model_name = model_info[\"model_name\"]\n",
        "            self.engine = LLaMAInferenceEngine(client=together_client, model_name=self.model_name)\n",
        "        elif self.api == \"huggingface\":\n",
        "            self.model_name = model_info[\"model_name\"]\n",
        "            self.engine = HuggingFaceInferenceEngine(model_path=self.model_name, device=device)\n",
        "        elif self.api == \"friendli\":\n",
        "            # Handle different Friendli deployment types\n",
        "            endpoint_type = model_info.get(\"endpoint_type\", \"serverless\")\n",
        "            \n",
        "            if endpoint_type == \"serverless\":\n",
        "                # Public models on serverless infrastructure\n",
        "                self.model_name = model_info[\"model_name\"]\n",
        "                self.engine = FriendliInferenceEngine(\n",
        "                    token=FRIENDLI_API_TOKEN,\n",
        "                    model_name=self.model_name\n",
        "                )\n",
        "            elif endpoint_type == \"dedicated\":\n",
        "                # Custom models on dedicated endpoints\n",
        "                self.model_name = model_info.get(\"endpoint_id\", \"\")\n",
        "                self.engine = FriendliInferenceEngine(\n",
        "                    token=FRIENDLI_API_TOKEN,\n",
        "                    endpoint_id=model_info.get(\"endpoint_id\"),\n",
        "                    team_id=model_info.get(\"team_id\"),\n",
        "                    adapter_route=model_info.get(\"adapter_route\")\n",
        "                )\n",
        "            elif endpoint_type == \"container\":\n",
        "                # Self-hosted container deployments\n",
        "                self.model_name = \"container\"\n",
        "                self.engine = FriendliInferenceEngine(\n",
        "                    token=None,  # Not needed for container\n",
        "                    base_url=model_info.get(\"base_url\"),\n",
        "                    adapter_route=model_info.get(\"adapter_name\"),\n",
        "                    use_container=True\n",
        "                )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported API type: {self.api}\")\n",
        "\n",
        "    def generate_response(self, system_prompt, user_prompt):\n",
        "        \"\"\"Generate a response using the underlying inference engine.\"\"\"\n",
        "        return self.engine.generate_response(system_prompt, user_prompt)\n",
        "\n",
        "# Add a new HuggingFace model to the MODEL_CONFIGS list\n",
        "def add_huggingface_model(model_name, display_name=None):\n",
        "   \"\"\"\n",
        "   Add a HuggingFace model to the MODEL_CONFIGS list.\n",
        "  \n",
        "   Args:\n",
        "       model_name (str): The model path/name on HuggingFace Hub.\n",
        "       display_name (str, optional): A custom display name for the model.\n",
        "      \n",
        "   Returns:\n",
        "       dict: The model configuration dictionary.\n",
        "   \"\"\"\n",
        "   display_name = display_name or f\"HF_{model_name.split('/')[-1]}\"\n",
        "   model_config = {\n",
        "       \"name\": display_name,\n",
        "       \"type\": \"huggingface\",\n",
        "       \"model_name\": model_name,\n",
        "       \"api\": \"huggingface\"\n",
        "   }\n",
        "   MODEL_CONFIGS.append(model_config)\n",
        "   print(f\"Added HuggingFace model {model_name} to MODEL_CONFIGS\")\n",
        "   return model_config\n",
        "\n",
        "# Functions to add different types of Friendli models\n",
        "def add_friendli_serverless(model_name, display_name=None):\n",
        "    \"\"\"\n",
        "    Add a Friendli serverless endpoint model to the MODEL_CONFIGS list.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): The model name (e.g., \"meta-llama-3.1-8b-instruct\")\n",
        "        display_name (str, optional): A custom display name for the model.\n",
        "        \n",
        "    Returns:\n",
        "        dict: The model configuration dictionary.\n",
        "    \"\"\"\n",
        "    display_name = display_name or f\"Friendli_{model_name.replace('-', '_')}\"\n",
        "    model_config = {\n",
        "        \"name\": display_name,\n",
        "        \"type\": \"friendli_serverless\",\n",
        "        \"model_name\": model_name,\n",
        "        \"api\": \"friendli\",\n",
        "        \"endpoint_type\": \"serverless\"\n",
        "    }\n",
        "    MODEL_CONFIGS.append(model_config)\n",
        "    print(f\"Added Friendli serverless model {model_name} to MODEL_CONFIGS\")\n",
        "    return model_config\n",
        "\n",
        "def add_friendli_dedicated(endpoint_id, display_name=None, team_id=None, adapter_route=None):\n",
        "    \"\"\"\n",
        "    Add a Friendli dedicated endpoint model to the MODEL_CONFIGS list.\n",
        "    \n",
        "    Args:\n",
        "        endpoint_id (str): The endpoint ID for the dedicated model\n",
        "        display_name (str, optional): A custom display name for the model.\n",
        "        team_id (str, optional): The team ID for the dedicated endpoint\n",
        "        adapter_route (str, optional): Adapter route for Multi-LoRA models\n",
        "        \n",
        "    Returns:\n",
        "        dict: The model configuration dictionary.\n",
        "    \"\"\"\n",
        "    display_name = display_name or f\"Friendli_Custom_{endpoint_id}\"\n",
        "    model_config = {\n",
        "        \"name\": display_name,\n",
        "        \"type\": \"friendli_dedicated\",\n",
        "        \"endpoint_id\": endpoint_id,\n",
        "        \"api\": \"friendli\",\n",
        "        \"endpoint_type\": \"dedicated\",\n",
        "        \"team_id\": team_id,\n",
        "        \"adapter_route\": adapter_route\n",
        "    }\n",
        "    MODEL_CONFIGS.append(model_config)\n",
        "    print(f\"Added Friendli dedicated endpoint {endpoint_id} to MODEL_CONFIGS\")\n",
        "    return model_config\n",
        "\n",
        "def add_friendli_container(base_url, display_name=None, adapter_name=None):\n",
        "    \"\"\"\n",
        "    Add a Friendli container deployment model to the MODEL_CONFIGS list.\n",
        "    \n",
        "    Args:\n",
        "        base_url (str): The base URL for the container (e.g., \"http://0.0.0.0:8000\")\n",
        "        display_name (str, optional): A custom display name for the model.\n",
        "        adapter_name (str, optional): The adapter name for Multi-LoRA models\n",
        "        \n",
        "    Returns:\n",
        "        dict: The model configuration dictionary.\n",
        "    \"\"\"\n",
        "    display_name = display_name or f\"Friendli_Container\"\n",
        "    model_config = {\n",
        "        \"name\": display_name,\n",
        "        \"type\": \"friendli_container\",\n",
        "        \"base_url\": base_url,\n",
        "        \"api\": \"friendli\",\n",
        "        \"endpoint_type\": \"container\",\n",
        "        \"adapter_name\": adapter_name\n",
        "    }\n",
        "    MODEL_CONFIGS.append(model_config)\n",
        "    print(f\"Added Friendli container at {base_url} to MODEL_CONFIGS\")\n",
        "    return model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "add_huggingface_model(\"meta-llama/Llama-3.2-3B-Instruct\") # change the path for your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. public model (like Llama) on Friendli's serverless infrastructure\n",
        "add_friendli_serverless(\"meta-llama-3.1-8b-instruct\")\n",
        "\n",
        "# 2. custom model on a dedicated endpoint\n",
        "add_friendli_dedicated(\n",
        "    endpoint_id=\"your-endpoint-id\",  # Replace with your actual endpoint ID\n",
        "    team_id=\"your-team-id\"           # Replace with your team ID if needed\n",
        ")\n",
        "\n",
        "# 3. custom model with LoRA adapter on a dedicated endpoint\n",
        "add_friendli_dedicated(\n",
        "    endpoint_id=\"your-endpoint-id\",\n",
        "    adapter_route=\"your-adapter-route\"\n",
        ")\n",
        "\n",
        "# 4. self-hosted Friendli Container deployment\n",
        "add_friendli_container(\"http://your-container-url:8000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AfaOWlUW39K",
        "outputId": "94227ba0-7cd1-4a71-e12c-c2a523363cd5"
      },
      "outputs": [],
      "source": [
        "# Initialize all models\n",
        "initialized_models_info = initialize_models(MODEL_CONFIGS, device)\n",
        "inference_engines = [InferenceEngine(model_info) for model_info in initialized_models_info]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-TAiMXEW5ww"
      },
      "outputs": [],
      "source": [
        "# Define the base Task class\n",
        "class Task:\n",
        "    def __init__(self, inference_engine, file_path, col_question, col_label, csv_output_path, json_output_path):\n",
        "        \"\"\"\n",
        "        Initializes a task with the given parameters.\n",
        "\n",
        "        Args:\n",
        "            inference_engine (InferenceEngine): The inference engine to generate responses.\n",
        "            file_path (str): Path to the CSV file with question data.\n",
        "            col_question (str): Name of the column containing the questions.\n",
        "            col_label (str): Name of the column containing the labels/answers.\n",
        "            csv_output_path (str): Path to save CSV output.\n",
        "            json_output_path (str): Path to save JSON output.\n",
        "        \"\"\"\n",
        "        self.inference = inference_engine\n",
        "        self.file_path = file_path\n",
        "        self.col_question = col_question\n",
        "        self.col_label = col_label\n",
        "        self.csv_output_path = csv_output_path\n",
        "        self.json_output_path = json_output_path\n",
        "\n",
        "    def save_results(self, answers, results):\n",
        "        \"\"\"\n",
        "        Saves task results to CSV and JSON files.\n",
        "\n",
        "        Args:\n",
        "            answers (list): List of answers generated for each question.\n",
        "            results (list): List of [question, true_label, generated_answer] for CSV output.\n",
        "        \"\"\"\n",
        "        # Save to JSON\n",
        "        with open(self.json_output_path, 'w', encoding='utf-8') as json_file:\n",
        "            json.dump(answers, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Save to CSV\n",
        "        df = pd.DataFrame(results, columns=[\"Question\", \"True Answer\", \"Generated Answer\"])\n",
        "        df.to_csv(self.csv_output_path, index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IJtS9APJSIkT"
      },
      "outputs": [],
      "source": [
        "# CDM Task Implementation\n",
        "class CommonDomainModelTask(Task):\n",
        "    def create_input_template(self, text):\n",
        "        \"\"\"Format the input template for the CDM task.\"\"\"\n",
        "        return f\"Provide a concise answer to the following question related to the Fintech Open Source Foundation's (FINOS) Common Domain Model (CDM): {text}\"\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Perform the Common Domain Model (CDM) task.\"\"\"\n",
        "        system_prompt = \"Deliver precise responses to questions about the Fintech Open Source Foundation's (FINOS) Common Domain Model (CDM).\"\n",
        "\n",
        "        answers, results = [], []\n",
        "        data = pd.read_csv(self.file_path)\n",
        "        x_test = data[self.col_question].astype(str).str.strip()\n",
        "        y_true = data[self.col_label].astype(str).str.strip()\n",
        "\n",
        "        for i in tqdm(range(len(x_test)), desc=\"CommonDomainModelTask\"):\n",
        "            question = x_test[i]\n",
        "            user_prompt = self.create_input_template(question)\n",
        "\n",
        "            try:\n",
        "                response = self.inference.generate_response(system_prompt, user_prompt)\n",
        "                answer = response\n",
        "            except Exception as e:\n",
        "                answer = f\"Error: {str(e)}\"\n",
        "\n",
        "            answers.append(answer)\n",
        "            results.append([x_test[i], y_true[i], answer])\n",
        "\n",
        "        self.save_results(answers, results)\n",
        "\n",
        "# MOF Task Implementation\n",
        "class MOFTask(Task):\n",
        "    def create_input_template(self, text, task_type=\"general\"):\n",
        "        \"\"\"Format the input template for the MOF task.\"\"\"\n",
        "        if task_type == \"abbreviation\":\n",
        "            return f\"Expand the following MOF-related abbreviation into its full form: {text}.\"\n",
        "        elif task_type == \"approve\":\n",
        "            return f\"Is the following license OSI-approved: {text}?\"\n",
        "        else:\n",
        "            return f\"Provide a concise answer to the following question about MOF's licensing requirements: {text}\"\n",
        "\n",
        "    def mof_task_abbreviation(self, question):\n",
        "        \"\"\"Handle MOF abbreviation expansion tasks.\"\"\"\n",
        "        system_prompt = \"Act as the Model Openness Framework (MOF) expert, specializing in abbreviation expansion. Expand the following abbreviation into its most accurate full form within the MOF context. Provide only the full form, no explanations.\"\n",
        "        return self.inference.generate_response(system_prompt, question)\n",
        "\n",
        "    def mof_task_approve(self, question):\n",
        "        \"\"\"Handle MOF OSI approval status tasks.\"\"\"\n",
        "        system_prompt = \"Act as the Model Openness Framework (MOF) expert, providing answers to questions about whether the license is OSI-approved or not. Provide only yes or no in the answer.\"\n",
        "        return self.inference.generate_response(system_prompt, question)\n",
        "\n",
        "    def mof_task_general(self, question):\n",
        "        \"\"\"Handle general MOF-related tasks.\"\"\"\n",
        "        system_prompt = \"Act as the Model Openness Framework (MOF) expert, providing precise answers to questions about license requirements under the MOF and the content of the license. Respond accurately and concisely within the MOF context.\"\n",
        "        return self.inference.generate_response(system_prompt, question)\n",
        "\n",
        "    def run(self, task_type=\"general\"):\n",
        "        \"\"\"Run the MOF task based on specified subtask type.\"\"\"\n",
        "        answers, results = [], []\n",
        "        data = pd.read_csv(self.file_path)\n",
        "        x_test = data[self.col_question].astype(str).str.strip()\n",
        "        y_true = data[self.col_label].astype(str).str.strip()\n",
        "\n",
        "        for i in tqdm(range(len(x_test)), desc=\"MOFTask\"):\n",
        "            question = x_test[i].replace(\"\\n\", \"\").strip()\n",
        "            user_prompt = self.create_input_template(question, task_type)\n",
        "\n",
        "            try:\n",
        "                if task_type == \"approve\":\n",
        "                    answer = self.mof_task_approve(user_prompt)\n",
        "                elif task_type == \"abbreviation\":\n",
        "                    answer = self.mof_task_abbreviation(user_prompt)\n",
        "                else:\n",
        "                    answer = self.mof_task_general(user_prompt)\n",
        "            except Exception as e:\n",
        "                answer = f\"Error: {str(e)}\"\n",
        "\n",
        "            answers.append(answer)\n",
        "            results.append([x_test[i], y_true[i], answer])\n",
        "\n",
        "        self.save_results(answers, results)\n",
        "\n",
        "# XBRL Task Implementation\n",
        "class XBRLTask(Task):\n",
        "    def create_input_template(self, text):\n",
        "        \"\"\"Format the input template for the XBRL task.\"\"\"\n",
        "        return f\"Provide the exact answer to the following question: {text}?\"\n",
        "\n",
        "    def xbrl_task_term(self, question):\n",
        "        \"\"\"Handle XBRL terminology definition tasks.\"\"\"\n",
        "        system_prompt = \"Provide a precise and concise definition of the term related to financial data extraction and application through XBRL (eXtensible Business Reporting Language) filings. Summarize its purpose, benefits, and primary uses in standardizing and sharing financial information within a single paragraph.\"\n",
        "        return self.inference.generate_response(system_prompt, question)\n",
        "\n",
        "    def xbrl_task_finmath(self, core_question, formula_name, formula):\n",
        "        \"\"\"Handle financial math calculations within XBRL data.\"\"\"\n",
        "        # Step 1: Generate calculation explanation\n",
        "        system_prompt_cal = \"Provide precise answers to detailed questions about financial data extraction and application using XBRL (eXtensible Business Reporting Language) filings, a standardized digital format for sharing and analyzing financial information. This task is to perform financial math. Ensure responses strictly match the correct answer without additional explanation. When answering questions about XBRL, it's essential to follow a structured approach. Here's how to methodically address these types of questions:\"\n",
        "        \n",
        "        user_prompt_cal = (\n",
        "            f\"Question: {core_question}\\n\"\n",
        "            f\"Formula Name: {formula_name}\\n\"\n",
        "            f\"Formula: {formula}\\n\"\n",
        "        )\n",
        "        \n",
        "        calculation = self.inference.generate_response(system_prompt_cal, user_prompt_cal)\n",
        "        \n",
        "        # Step 2: Generate final answer\n",
        "        system_prompt_ans = \"You are a financial expert tasked with carefully reading, analyzing, and answering the following eXtensible Business Reporting Language. Please follow the steps below:\"\n",
        "        \n",
        "        user_prompt_ans = (\n",
        "            f\"Your task is to read the eXtensible Business Reporting Language (XBRL) question: {core_question} and find \"\n",
        "            f\"the final answer based on the explanation provided: {calculation}. Provide only the final answer which is \"\n",
        "            f\"the numerical result of the calculation. For formulas like ROI, provide percentages. Never use the percent symbol in percentages.\"\n",
        "        )\n",
        "        \n",
        "        final_answer = self.inference.generate_response(system_prompt_ans, user_prompt_ans)\n",
        "        return final_answer\n",
        "\n",
        "    def xbrl_task_other(self, question):\n",
        "        \"\"\"Handle domain-specific, numeric, and tag queries.\"\"\"\n",
        "        system_prompt = \"Provide precise answers to detailed questions about financial data extraction and application using XBRL (eXtensible Business Reporting Language) filings, a standardized digital format for sharing and analyzing financial information. This task covers three areas: domain-specific queries, numeric queries, and providing the correct US GAAP XBRL tags (e.g., 'US GAAP XBRL tag for revenue' should be answered as 'us-gaap:RevenueFromContractWithCustomerExcludingAssessedTax'). Ensure responses strictly match the correct answer without additional explanation.\"\n",
        "        return self.inference.generate_response(system_prompt, question)\n",
        "\n",
        "    def run(self, task_type=\"other\"):\n",
        "        \"\"\"Run the XBRL task based on specified subtask type.\"\"\"\n",
        "        answers, results = [], []\n",
        "        \n",
        "        # Determine separator based on file extension\n",
        "        _, file_extension = os.path.splitext(self.file_path)\n",
        "        sep = '\\t' if file_extension.lower() == '.tsv' else ','\n",
        "        \n",
        "        # Read data\n",
        "        data = pd.read_csv(self.file_path, sep=sep)\n",
        "        x_test = data[self.col_question].astype(str).str.strip()\n",
        "        y_true = data[self.col_label].astype(str).str.strip()\n",
        "        \n",
        "        # Get formula columns for finmath tasks\n",
        "        if task_type == \"finmath\":\n",
        "            required_columns = [\"Formula Name\", \"Formula\"]\n",
        "            for col in required_columns:\n",
        "                if col not in data.columns:\n",
        "                    raise ValueError(f\"The required column '{col}' is missing in the input file.\")\n",
        "                    \n",
        "            formula_name_col = data[\"Formula Name\"].astype(str).str.strip()\n",
        "            formula_col = data[\"Formula\"].astype(str).str.strip()\n",
        "        else:\n",
        "            formula_name_col = None\n",
        "            formula_col = None\n",
        "            \n",
        "        for i in tqdm(range(len(x_test)), desc=\"XBRLTask\"):\n",
        "            question = str(x_test[i]).replace(\"\\n\", \"\").strip()\n",
        "            \n",
        "            try:\n",
        "                if task_type == \"finmath\":\n",
        "                    formula_name = str(formula_name_col[i]).strip()\n",
        "                    formula = str(formula_col[i]).strip()\n",
        "                    user_prompt = self.create_input_template(question)\n",
        "                    answer = self.xbrl_task_finmath(question, formula_name, formula)\n",
        "                elif task_type == \"term\":\n",
        "                    user_prompt = self.create_input_template(question)\n",
        "                    answer = self.xbrl_task_term(user_prompt)\n",
        "                else:  # 'other' for domain, numeric, tag queries\n",
        "                    user_prompt = self.create_input_template(question)\n",
        "                    answer = self.xbrl_task_other(user_prompt)\n",
        "            except Exception as e:\n",
        "                answer = f\"Error: {str(e)}\"\n",
        "                \n",
        "            answers.append(answer)\n",
        "            results.append([x_test[i], y_true[i], answer])\n",
        "            \n",
        "        self.save_results(answers, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpaRniLyXYPW"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "def main():\n",
        "    # Create output directories\n",
        "    save_val_dir = \"inference_results\"\n",
        "    save_path_csv = os.path.join(save_val_dir, \"csv\")\n",
        "    save_path_json = os.path.join(save_val_dir, \"json\")\n",
        "    os.makedirs(save_path_csv, exist_ok=True)\n",
        "    os.makedirs(save_path_json, exist_ok=True)\n",
        "\n",
        "    # Define task configurations\n",
        "    tasks = [\n",
        "        # XBRL Tasks\n",
        "        {\n",
        "            \"class\": XBRLTask,\n",
        "            \"file_path\": \"test_data/Task7-XBRL Terminology-testing.csv\",\n",
        "            \"col_question\": \"Term\",\n",
        "            \"col_label\": \"Explanation\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"xbrl_task_term.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"xbrl_task_term.json\"),\n",
        "            \"params\": {\"task_type\": \"term\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": XBRLTask,\n",
        "            \"file_path\": \"test_data/Task7-XBRL-domain_numberic_query-testing.csv\",\n",
        "            \"col_question\": \"question\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"xbrl_task_numeric_query.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"xbrl_task_numeric_query.json\"),\n",
        "            \"params\": {\"task_type\": \"other\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": XBRLTask,\n",
        "            \"file_path\": \"test_data/Task7-XBRL-financial_formula_calc-XBRL-tags-testing.csv\",\n",
        "            \"col_question\": \"query\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"xbrl_task_formula_tag.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"xbrl_task_formula_tag.json\"),\n",
        "            \"params\": {\"task_type\": \"tag_ratio\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": XBRLTask,\n",
        "            \"file_path\": \"test_data/Task7-XBRL-financial_math-testing_format_limited.tsv\",\n",
        "            \"col_question\": \"Question\",\n",
        "            \"col_label\": \"Answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"xbrl_task_finmath.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"xbrl_task_finmath.json\"),\n",
        "            \"params\": {\"task_type\": \"finmath\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": XBRLTask,\n",
        "            \"file_path\": \"test_data/Task7-XBRL-Tag_Query_to_XBRL_Reports-testing.csv\",\n",
        "            \"col_question\": \"query\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"xbrl_task_tag_query.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"xbrl_task_tag_query.json\"),\n",
        "            \"params\": {\"task_type\": \"other\"}\n",
        "        },\n",
        "        # Common Domain Model Task\n",
        "        {\n",
        "            \"class\": CommonDomainModelTask,\n",
        "            \"file_path\": \"test_data/Task8-CDM-testing.csv\",\n",
        "            \"col_question\": \"Question\",\n",
        "            \"col_label\": \"Answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"cdm_task.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"cdm_task.json\"),\n",
        "            \"params\": {}\n",
        "        },\n",
        "        # MOF Tasks\n",
        "        {\n",
        "            \"class\": MOFTask,\n",
        "            \"file_path\": \"test_data/Task9_MOF-License_OSI_Approval-testing.csv\",\n",
        "            \"col_question\": \"Question\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"mof_task_approval.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"mof_task_approval.json\"),\n",
        "            \"params\": {\"task_type\": \"approve\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": MOFTask,\n",
        "            \"file_path\": \"test_data/Task9-MOF-Detailed_QA-testing.csv\",\n",
        "            \"col_question\": \"Question\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"mof_task_detailed.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"mof_task_detailed.json\"),\n",
        "            \"params\": {\"task_type\": \"general\"}\n",
        "        },\n",
        "        {\n",
        "            \"class\": MOFTask,\n",
        "            \"file_path\": \"test_data/Task9-MOF-License_Abbreviation-testing.csv\",\n",
        "            \"col_question\": \"Question\",\n",
        "            \"col_label\": \"answer\",\n",
        "            \"csv_output_path\": os.path.join(save_path_csv, \"mof_task_abbreviation.csv\"),\n",
        "            \"json_output_path\": os.path.join(save_path_json, \"mof_task_abbreviation.json\"),\n",
        "            \"params\": {\"task_type\": \"abbreviation\"}\n",
        "        },\n",
        "    ]\n",
        "\n",
        "  # Loop through each task and run inference for all models\n",
        "    for task_config in tasks:\n",
        "        task_class = task_config[\"class\"]\n",
        "        for engine in inference_engines:\n",
        "            model_name = engine.name\n",
        "            print(f\"\\nRunning task '{task_class.__name__}' with model '{model_name}'\")\n",
        "\n",
        "            # Create output file paths with model name suffix\n",
        "            csv_output_path = task_config[\"csv_output_path\"].replace(\".csv\", f\"_{model_name}.csv\")\n",
        "            json_output_path = task_config[\"json_output_path\"].replace(\".json\", f\"_{model_name}.json\")\n",
        "\n",
        "            # Initialize the task with the current inference engine and updated output paths\n",
        "            task_instance = task_class(\n",
        "                inference_engine=engine,\n",
        "                file_path=task_config[\"file_path\"],\n",
        "                col_question=task_config[\"col_question\"],\n",
        "                col_label=task_config[\"col_label\"],\n",
        "                csv_output_path=csv_output_path,\n",
        "                json_output_path=json_output_path\n",
        "            )\n",
        "\n",
        "            # Run the task with any additional parameters\n",
        "            task_instance.run(**task_config.get(\"params\", {}))\n",
        "\n",
        "    print(\"\\nAll tasks completed for all models.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbjSUvUYXZ3_",
        "outputId": "f568d28b-d5f5-4733-abca-f0ce9601b545"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeVC1GMLXxVq",
        "outputId": "b0e701a4-b2f6-41d9-89de-2fe5577147bc"
      },
      "outputs": [],
      "source": [
        "!zip -r inference_results.zip inference_results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mV154j2pYLJ9",
        "outputId": "6ce367ab-4981-423f-fb3d-5549e4fcffcf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"inference_results.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr3PGhkBerjl"
      },
      "outputs": [],
      "source": [
        "!rm -r inference_results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIBXnaQ5evzU"
      },
      "outputs": [],
      "source": [
        "!rm -r test_data/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
